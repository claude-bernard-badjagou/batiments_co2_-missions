
# Application Streamlit : Pr√©dicteur de CO‚ÇÇ pour b√¢timents (version monofichier)

# 1) --- Importations ---
import os                                   # Chargeons os pour manipuler les chemins de fichiers et tester la pr√©sence des ressources
import io                                   # Chargeons io pour g√©rer des flux m√©moire (export binaire des mod√®les)
import joblib                               # Chargeons joblib pour s√©rialiser/d√©s√©rialiser le mod√®le et le scaler
import numpy as np                          # Chargeons numpy pour les calculs num√©riques (tableaux, histogrammes)
import pandas as pd                         # Chargeons pandas pour charger et manipuler le tableau de donn√©es
import streamlit as st                      # Chargeons Streamlit pour construire l'interface web
from typing import Tuple, Optional, List    # Chargeons les types pour annoter proprement les fonctions

# Importations scikit-learn pour pr√©traitement, mod√®le et m√©triques
from sklearn.model_selection import train_test_split   # Chargeons train_test_split pour cr√©er un jeu d'entra√Ænement/test
from sklearn.preprocessing import StandardScaler       # Chargeons StandardScaler pour mettre √† l'√©chelle les variables explicatives
from sklearn.ensemble import RandomForestRegressor     # Chargeons RandomForestRegressor pour construire un mod√®le robuste et non lin√©aire
from sklearn.metrics import r2_score, mean_squared_error  # Chargeons les m√©triques R¬≤ et RMSE pour √©valuer les performances

# 2) --- Constantes de ressources partag√©es ---
CHEMIN_DONNEES_PAR_DEFAUT = "df_resultat_analyse_batiments.csv"  # D√©finissons le nom du fichier CSV attendu
CHEMIN_MODELE = "model_co2.pkl"                                  # D√©finissons le nom du fichier mod√®le
CHEMIN_SCALER = "scaler_co2.pkl"                                 # D√©finissons le nom du fichier scaler

# 3) --- Colonnes par d√©faut (selon le code initial fourni) ---
COLONNES_FEATURES_PAR_DEFAUT: List[str] = [
    'Electricity(kBtu)',                    # D√©finissons la colonne de consommation √©lectrique (kBtu)
    'NaturalGas(kBtu)',                     # D√©finissons la colonne de consommation de gaz (kBtu)
    'BuildingAge',                          # D√©finissons la colonne d'√¢ge du b√¢timent
    'Prop_LargestPropertyUseTypeGFA'        # D√©finissons la proportion de la surface principale
]
COLONNE_CIBLE_PAR_DEFAUT = 'TotalGHGEmissions'          # D√©finissons la colonne cible : √©missions de GES totales

# 4) --- Config de page Streamlit ---
st.set_page_config(                         # Configurons la page pour un affichage large et un titre explicite
    page_title="Pr√©dicteur CO‚ÇÇ des b√¢timents",
    page_icon="üåç",
    layout="wide"
)

# 5) --- Fonctions utilitaires (chargement, entra√Ænement, pr√©diction) ---
@st.cache_data(show_spinner=False)
def charger_donnees(chemin: Optional[str] = None, fichier_televerse: Optional[io.BytesIO] = None) -> Optional[pd.DataFrame]:
    """Chargeons les donn√©es en priorit√© depuis le fichier fourni par l'utilisateur,
    sinon depuis le chemin par d√©faut, pour permettre l'exploration et l'entra√Ænement."""
    # Si l'utilisateur a t√©l√©vers√© un fichier via l'interface, nous l'utilisons en priorit√©
    if fichier_televerse is not None:
        try:
            df = pd.read_csv(fichier_televerse)      # Lisons le CSV t√©l√©vers√© pour r√©cup√©rer les donn√©es tabulaires
            return df                                # Renvoyons le DataFrame pour la suite de l'application
        except Exception as e:
            st.error(f"Erreur de lecture du CSV t√©l√©vers√© : {e}")  # Affichons une erreur en cas de probl√®me de parsing
            return None

    # Sinon, tentons de lire depuis le chemin fourni ou celui par d√©faut
    chemin_effectif = chemin or CHEMIN_DONNEES_PAR_DEFAUT  # D√©terminons le chemin effectif √† utiliser
    if os.path.exists(chemin_effectif):
        try:
            df = pd.read_csv(chemin_effectif)        # Lisons le CSV local pour charger les donn√©es
            return df                                # Renvoyons le DataFrame si tout va bien
        except Exception as e:
            st.error(f"Erreur de lecture de '{chemin_effectif}' : {e}")  # Signalons l'exception
            return None
    else:
        return None                                   # Si le fichier n'existe pas, renvoyons None (g√©r√© par l'UI)


def preparer_X_y(
    df: pd.DataFrame,
    colonnes_features: List[str],
    colonne_cible: str
) -> Tuple[pd.DataFrame, pd.Series]:
    """Pr√©parons X et y en v√©rifiant la pr√©sence des colonnes n√©cessaires pour l'entra√Ænement/pr√©diction."""
    # V√©rifions que toutes les colonnes features existent dans le DataFrame
    manquantes = [c for c in colonnes_features if c not in df.columns]
    if manquantes:
        raise ValueError(f"Colonnes explicatives manquantes : {manquantes}")  # Alertons si des colonnes sont absentes

    # V√©rifions que la colonne cible existe
    if colonne_cible not in df.columns:
        raise ValueError(f"Colonne cible manquante : {colonne_cible}")       # Alertons si la cible est absente

    # S√©lectionnons X (features) et y (cible)
    X = df[colonnes_features].copy()                 # Copions X pour √©viter de modifier le DataFrame original
    y = df[colonne_cible].copy()                     # Copions y pour coh√©rence

    # Convertissons les colonnes en num√©rique si n√©cessaire (erreurs coercitives deviennent NaN)
    X = X.apply(pd.to_numeric, errors='coerce')      # For√ßons les types num√©riques pour le mod√®le
    y = pd.to_numeric(y, errors='coerce')            # For√ßons y en num√©rique pour la m√©trique

    # Supprimons les lignes avec NaN dans X ou y pour garantir un entra√Ænement propre
    masque_valide = X.notna().all(axis=1) & y.notna()
    X = X[masque_valide]
    y = y[masque_valide]

    return X, y                                      # Renvoyons X et y pr√™ts √† l'emploi


@st.cache_resource(show_spinner=False)
def charger_ou_entrainer_modele(
    df: Optional[pd.DataFrame],
    colonnes_features: List[str] = COLONNES_FEATURES_PAR_DEFAUT,
    colonne_cible: str = COLONNE_CIBLE_PAR_DEFAUT,
    n_estimateurs: int = 300,
    profondeur_max: Optional[int] = None,
    graine: int = 42
):
    """Chargeons le mod√®le et le scaler depuis les fichiers partag√©s s'ils existent ;
    sinon, entra√Ænons un nouveau mod√®le √† partir du CSV (si disponible) et sauvegardons-les."""
    # Tentons d'abord de charger les artefacts existants pour √©viter de r√©entra√Æner syst√©matiquement
    if os.path.exists(CHEMIN_MODELE) and os.path.exists(CHEMIN_SCALER):
        try:
            modele = joblib.load(CHEMIN_MODELE)       # Chargeons le mod√®le entra√Æn√© depuis le fichier .pkl
            scaler = joblib.load(CHEMIN_SCALER)       # Chargeons le scaler entra√Æn√© depuis le fichier .pkl
            return modele, scaler, None               # Renvoyons les objets sans m√©triques (pas de r√©entra√Ænement)
        except Exception as e:
            st.warning(f"Impossible de charger les artefacts existants : {e}. Entra√Ænement pr√©vu.")

    # Si l'on ne peut charger, essayons d'entra√Æner si des donn√©es sont disponibles
    if df is None:
        return None, None, None                       # Renvoyons None si nous ne pouvons pas entra√Æner faute de donn√©es

    # Pr√©parons X et y en se basant sur les colonnes par d√©faut (ou personnalis√©es)
    X, y = preparer_X_y(df, colonnes_features, colonne_cible)

    # S√©parons entra√Ænement/test pour √©valuer le mod√®le de mani√®re honn√™te
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=graine
    )

    # Cr√©ons et ajustons le scaler sur l'entra√Ænement (et transformons train/test)
    scaler = StandardScaler()                         # Instancions le scaler pour normaliser les variables explicatives
    X_train_s = scaler.fit_transform(X_train)         # Ajustons le scaler et transformons X_train pour centrer/r√©duire
    X_test_s = scaler.transform(X_test)               # Transformons X_test avec les m√™mes param√®tres appris

    # Instancions le mod√®le RandomForestRegressor avec des hyperparam√®tres raisonnables
    modele = RandomForestRegressor(
        n_estimators=n_estimateurs,
        max_depth=profondeur_max,
        random_state=graine,
        n_jobs=-1
    )

    # Entra√Ænons le mod√®le sur les donn√©es mises √† l'√©chelle
    modele.fit(X_train_s, y_train)

    # √âvaluons les performances pour information (R¬≤ et RMSE)
    pred_test = modele.predict(X_test_s)              # Produisons des pr√©dictions sur le test
    r2 = r2_score(y_test, pred_test)                  # Calculons le coefficient de d√©termination R¬≤
    rmse = mean_squared_error(y_test, pred_test, squared=False)  # Calculons la racine de l'erreur quadratique moyenne

    # Sauvegardons les artefacts entra√Æn√©s pour r√©utilisation
    joblib.dump(modele, CHEMIN_MODELE)                # √âcrivons le mod√®le sur disque pour un usage ult√©rieur
    joblib.dump(scaler, CHEMIN_SCALER)                # √âcrivons le scaler sur disque pour un usage ult√©rieur

    return modele, scaler, {"r2": r2, "rmse": rmse}  # Renvoyons le mod√®le, le scaler et les m√©triques


def predire_co2(
    modele: RandomForestRegressor,
    scaler: StandardScaler,
    tableau_features: np.ndarray
) -> float:
    """Appliquons le scaler puis le mod√®le pour pr√©dire une valeur de CO‚ÇÇ √† partir d'un vecteur de features."""
    donnees_mises_echelle = scaler.transform(tableau_features)  # Transformons les features avec le scaler appris
    prediction = modele.predict(donnees_mises_echelle)           # Appliquons le mod√®le pour obtenir la pr√©diction
    return float(prediction[0])                                  # Convertissons la pr√©diction en float natif

# 6) --- Interface : Barre de navigation horizontale (onglets) ---
# Cr√©ons des onglets pour repr√©senter les pages majeures de l'application
onglet_accueil, onglet_exploration, onglet_modele, onglet_prediction, onglet_api = st.tabs([
    "üè† Accueil",
    "üìä Exploration",
    "üîß Pretraitement & Modele",
    "ü§ñ Prediction",
    "üì° API & Export"
])

# 7) --- Gestion initiale des donn√©es (disponibles globalement) ---
with st.sidebar:
    # Proposons le t√©l√©versement du CSV si l'utilisateur ne souhaite pas s'appuyer sur le fichier local
    st.markdown("### üì• Donn√©es")
    fichier_csv_televerse = st.file_uploader(
        "T√©l√©verser df_resultat_analyse_batiments.csv (optionnel)", type=["csv"]
    )

# Chargeons les donn√©es soit depuis l'upload, soit depuis le fichier local par d√©faut
df_global = charger_donnees(CHEMIN_DONNEES_PAR_DEFAUT, fichier_csv_televerse)

# 8) --- Onglet Accueil ---
with onglet_accueil:
    st.markdown("""
    # üåç Pr√©dicteur de CO‚ÇÇ des b√¢timents

    **But** : Estimer rapidement et de mani√®re fiable les **√©missions de CO‚ÇÇ** d'un b√¢timent
    √† partir de variables explicatives simples (consommation d'√©lectricit√© et de gaz, √¢ge, proportion de surface principale).

    **Ressources partag√©es** utilis√©es par d√©faut :
    - Donn√©es : `df_resultat_analyse_batiments.csv`
    - Mod√®le : `model_co2.pkl`
    - Scaler : `scaler_co2.pkl`

    **Navigation** : utilisez la barre horizontale en haut pour parcourir :
    1. *Exploration* des donn√©es (profilage rapide)
    2. *Pr√©traitement & Mod√®le* (entra√Ænement, m√©triques et importances)
    3. *Pr√©diction* (formulaire interactif)
    4. *API & Export* (documentation et t√©l√©chargement des artefacts)
    """)

    # Affichons un rappel sur la disponibilit√© des donn√©es
    if df_global is None:
        st.info(
            "Aucune donn√©e locale d√©tect√©e. T√©l√©versez le CSV dans la barre lat√©rale ou placez `df_resultat_analyse_batiments.csv` √† la racine."
        )
    else:
        st.success("Donn√©es charg√©es avec succ√®s : `df_resultat_analyse_batiments.csv`.")

# 9) --- Onglet Exploration ---
with onglet_exploration:
    st.markdown("## üìä Exploration des donn√©es")

    if df_global is None:
        st.warning("Exploration indisponible : aucune donn√©e charg√©e.")
    else:
        # Affichons un aper√ßu et des infos g√©n√©rales
        st.markdown("### Aper√ßu du jeu de donn√©es")
        st.dataframe(df_global.head(20), width="stretch")  # Affichons les 20 premi√®res lignes (width='stretch' remplace use_container_width)

        st.markdown("### Statistiques descriptives (num√©riques)")
        st.dataframe(df_global.describe(include='number'), width="stretch")  # R√©sumons les colonnes num√©riques

        st.markdown("### Valeurs manquantes par colonne")
        manquants = df_global.isna().sum().sort_values(ascending=False)  # Comptons les NaN pour chaque colonne
        st.dataframe(manquants.to_frame("nb_nan"), width="stretch")       # Affichons sous forme de tableau

        # Choix de colonnes num√©riques pour des graphiques rapides
        colonnes_numeriques = df_global.select_dtypes(include='number').columns.tolist()  # R√©cup√©rons les colonnes num√©riques disponibles
        if colonnes_numeriques:
            c1, c2 = st.columns(2)                    # Cr√©ons deux colonnes pour juxtaposer les visuels
            with c1:
                st.markdown("#### Histogramme rapide")
                col_hist = st.selectbox("S√©lectionnez une colonne num√©rique", colonnes_numeriques)  # Laissons l'utilisateur choisir la variable
                try:
                    # Construisons un histogramme simple avec numpy
                    valeurs = df_global[col_hist].dropna().values           # R√©cup√©rons les valeurs non manquantes
                    if len(valeurs) > 0:
                        nb_bins = st.slider("Nombre de classes (bins)", 10, 100, 30)
                        histo, bords = np.histogram(valeurs, bins=nb_bins)             # Calculons l'histogramme
                        df_histo = pd.DataFrame({"classe": bords[:-1], "frequence": histo})  # Convertissons pour affichage
                        st.bar_chart(df_histo.set_index("classe"))                      # Affichons le graphique en barres
                    else:
                        st.info("Aucune valeur disponible pour tracer l'histogramme.")
                except Exception as e:
                    st.error(f"Erreur histogramme : {e}")

            with c2:
                st.markdown("#### Nuage de points (x vs y)")
                if len(colonnes_numeriques) >= 2:
                    x_col = st.selectbox("Axe X", colonnes_numeriques, index=0)      # Choisissons l'axe X
                    y_col = st.selectbox("Axe Y", colonnes_numeriques, index=1)      # Choisissons l'axe Y
                    try:
                        st.scatter_chart(df_global[[x_col, y_col]].dropna(), x=x_col, y=y_col)  # Tra√ßons un nuage de points
                    except Exception as e:
                        st.error(f"Erreur nuage de points : {e}")
                else:
                    st.info("Nuage de points indisponible : moins de deux colonnes num√©riques.")

        # Corr√©lations (si suffisamment de colonnes num√©riques)
        if len(colonnes_numeriques) >= 2:
            st.markdown("### Matrice de corr√©lation (num√©rique)")
            try:
                corr = df_global[colonnes_numeriques].corr()  # Calculons les corr√©lations lin√©aires entre variables num√©riques
                st.dataframe(corr, width="stretch")           # Affichons la matrice (width='stretch')
            except Exception as e:
                st.error(f"Erreur corr√©lation : {e}")

# 10) --- Onglet Pr√©traitement & Mod√®le ---
with onglet_modele:
    st.markdown("## üîß Pr√©traitement & Mod√®le")

    if df_global is None:
        st.warning("Pr√©traitement indisponible : aucune donn√©e charg√©e.")
    else:
        # Param√©trage des colonnes et hyperparam√®tres
        st.markdown("### Param√®tres de pr√©paration et d'entra√Ænement")
        colonnes_proposees = [c for c in COLONNES_FEATURES_PAR_DEFAUT if c in df_global.columns]  # Filtrons les features disponibles
        if len(colonnes_proposees) < len(COLONNES_FEATURES_PAR_DEFAUT):
            st.warning("Certaines colonnes par d√©faut manquent dans le CSV. Adaptez la s√©lection ci-dessous si n√©cessaire.")

        colonnes_features = st.multiselect(
            "Colonnes explicatives",
            options=df_global.columns.tolist(),
            default=colonnes_proposees
        )
        colonne_cible = st.selectbox(
            "Colonne cible (√©missions de GES)",
            options=df_global.columns.tolist(),
            index=df_global.columns.get_loc(COLONNE_CIBLE_PAR_DEFAUT) if COLONNE_CIBLE_PAR_DEFAUT in df_global.columns else 0
        )

        c1, c2, c3 = st.columns(3)
        with c1:
            n_est = st.number_input("n_estimators (arbres)", min_value=50, max_value=1000, value=300, step=50)
        with c2:
            # ‚ö†Ô∏è Correctif Streamlit Cloud : autoriser 0 pour que la valeur initiale 0 ne plante pas
            profondeur_max = st.number_input(
                "max_depth (None = automatique)",
                min_value=0, max_value=100, value=0, step=1, help="0 = None (illimit√©)"
            )
            profondeur_max = None if profondeur_max == 0 else int(profondeur_max)
        with c3:
            graine = st.number_input("random_state", min_value=0, max_value=10_000, value=42, step=1)

        # Entra√Ænement / Chargement
        st.markdown("### Entra√Ænement ou chargement du mod√®le")
        lancer_reentrainement = st.checkbox("R√©entra√Æner maintenant (√©crase les artefacts existants)")

        # Si l'utilisateur force le r√©entra√Ænement, nous l'ex√©cutons; sinon nous tentons de charger
        if lancer_reentrainement:
            try:
                X, y = preparer_X_y(df_global, colonnes_features, colonne_cible)
                modele, scaler, metriques = charger_ou_entrainer_modele(
                    df_global, colonnes_features, colonne_cible,
                    n_estimateurs=n_est, profondeur_max=profondeur_max, graine=graine
                )
                if metriques:
                    st.success(f"Mod√®le r√©entra√Æn√©. R¬≤ = {metriques['r2']:.4f} | RMSE = {metriques['rmse']:.4f}")
            except Exception as e:
                st.error(f"Erreur de r√©entra√Ænement : {e}")
        else:
            # Tentons de charger, sinon entra√Ænons automatiquement si possible
            modele, scaler, metriques = charger_ou_entrainer_modele(
                df_global, colonnes_features, colonne_cible,
                n_estimateurs=n_est, profondeur_max=profondeur_max, graine=graine
            )
            if metriques:
                st.info(f"Mod√®le entra√Æn√© (artefacts absents). R¬≤ = {metriques['r2']:.4f} | RMSE = {metriques['rmse']:.4f}")

        # Affichons les importances si un mod√®le est disponible et compatible
        if isinstance(modele, RandomForestRegressor) and len(colonnes_features) == len(modele.feature_importances_):
            st.markdown("### Importance des variables (Random Forest)")
            importances = pd.Series(modele.feature_importances_, index=colonnes_features).sort_values(ascending=False)
            st.bar_chart(importances)
        else:
            st.info("Importances non disponibles.")

# 11) --- Onglet Pr√©diction ---
with onglet_prediction:
    st.markdown("## ü§ñ Pr√©diction interactive des √©missions de CO‚ÇÇ")

    # Assurons-nous que nous pouvons charger le mod√®le et le scaler existants (entra√Æn√©s ou charg√©s auparavant)
    modele_actuel, scaler_actuel, _ = charger_ou_entrainer_modele(df_global)

    if (modele_actuel is None) or (scaler_actuel is None):
        st.warning("Aucun mod√®le/scaler disponible. Rendez-vous dans l'onglet 'Pretraitement & Modele' pour entra√Æner/charger.")
    else:
        # D√©finissons des valeurs par d√©faut (m√©dianes) si les donn√©es sont disponibles, sinon des valeurs g√©n√©riques
        if df_global is not None and all(c in df_global.columns for c in COLONNES_FEATURES_PAR_DEFAUT):
            med_elec = float(pd.to_numeric(df_global['Electricity(kBtu)'], errors='coerce').median())
            med_gaz  = float(pd.to_numeric(df_global['NaturalGas(kBtu)'], errors='coerce').median())
            med_age  = float(pd.to_numeric(df_global['BuildingAge'], errors='coerce').median())
            med_prop = float(pd.to_numeric(df_global['Prop_LargestPropertyUseTypeGFA'], errors='coerce').median())
        else:
            med_elec, med_gaz, med_age, med_prop = 5_000.0, 1_000.0, 30.0, 0.6

        c1, c2, c3, c4 = st.columns(4)
        with c1:
            conso_electricite = st.number_input("Consommation √©lectrique (kBtu)", min_value=0.0, value=med_elec, step=100.0)
        with c2:
            conso_gaz = st.number_input("Consommation gaz (kBtu)", min_value=0.0, value=med_gaz, step=50.0)
        with c3:
            age_batiment = st.number_input("√Çge du b√¢timent (ann√©es)", min_value=0.0, value=med_age, step=1.0)
        with c4:
            prop_surface = st.number_input("Prop. surface principale (0-1)", min_value=0.0, max_value=1.0, value=med_prop, step=0.01)

        if st.button("Pr√©dire les √©missions de CO‚ÇÇ"):
            try:
                # Construisons le vecteur de features dans l'ordre d'entra√Ænement par d√©faut
                vecteur = np.array([[conso_electricite, conso_gaz, age_batiment, prop_surface]], dtype=float)
                valeur_predite = predire_co2(modele_actuel, scaler_actuel, vecteur)
                st.success(f"√âmissions de CO‚ÇÇ estim√©es : {valeur_predite:.2f} (unit√©s de la cible)")
            except Exception as e:
                st.error(f"Erreur de pr√©diction : {e}")

# 12) --- Onglet API & Export ---
with onglet_api:
    st.markdown("## üì° API & Export des artefacts")

    st.markdown("""
    ### Documentation rapide de l'API pr√©vue
    Nous exposons un point d'entr√©e **POST** `/predict` (FastAPI dans une application s√©par√©e) acceptant le JSON suivant :

    ```json
    {
      "electricity": 5000.0,
      "gas": 1000.0,
      "age": 30.0,
      "prop_gfa": 0.6
    }
    ```

    La r√©ponse attendue :
    ```json
    { "prediction_CO2": 123.45 }
    ```

    > Remarque : dans cette version **monofichier** Streamlit, l'API n'est pas d√©marr√©e. 
    > Pour d√©ployer une API, conservez les m√™mes ressources partag√©es (`model_co2.pkl`, `scaler_co2.pkl`) 
    > et d√©marrez un service FastAPI s√©par√© (ex. `uvicorn main:app --host 0.0.0.0 --port 8000`).
    """)

    # Proposons le t√©l√©chargement des artefacts si pr√©sents
    st.markdown("### T√©l√©chargement des artefacts (si disponibles)")

    col_a, col_b = st.columns(2)
    with col_a:
        if os.path.exists(CHEMIN_MODELE):
            try:
                with open(CHEMIN_MODELE, "rb") as f:
                    b = f.read()                               # Lisons le binaire du mod√®le pour le proposer en t√©l√©chargement
                st.download_button("T√©l√©charger model_co2.pkl", b, file_name="model_co2.pkl")
            except Exception as e:
                st.error(f"T√©l√©chargement mod√®le impossible : {e}")
        else:
            st.info("Aucun mod√®le trouv√© √† t√©l√©charger.")

    with col_b:
        if os.path.exists(CHEMIN_SCALER):
            try:
                with open(CHEMIN_SCALER, "rb") as f:
                    b = f.read()                               # Lisons le binaire du scaler pour le proposer en t√©l√©chargement
                st.download_button("T√©l√©charger scaler_co2.pkl", b, file_name="scaler_co2.pkl")
            except Exception as e:
                st.error(f"T√©l√©chargement scaler impossible : {e}")
        else:
            st.info("Aucun scaler trouv√© √† t√©l√©charger.")

    st.markdown("""
    ---
    **Bonnes pratiques** :
    - Conservez exactement les **m√™mes colonnes** et **le m√™me ordre** qu'√† l'entra√Ænement lors de la pr√©diction.
    - Appliquez **le m√™me scaler** (StandardScaler) appris sur les donn√©es d'entra√Ænement.
    - Mettez √† jour r√©guli√®rement le mod√®le si les conditions d'exploitation changent (nouvelles typologies de b√¢timents, nouvelles normes, etc.).
    """)
